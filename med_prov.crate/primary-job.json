{
    "polygons_geojson": {
        "class": "File",
        "location": "892adcb0b8895ca8ed0a264220e67e59b99a27cc",
        "size": 8253,
        "basename": "polygons.geojson",
        "nameroot": "polygons",
        "nameext": ".geojson",
        "checksum": "sha1$892adcb0b8895ca8ed0a264220e67e59b99a27cc"
    },
    "shoreline_geojson": {
        "class": "File",
        "location": "f5a39e6eb18a0acb427c1997ad5a2588899fd777",
        "size": 27838248,
        "basename": "shorelines.geojson",
        "nameroot": "shorelines",
        "nameext": ".geojson",
        "checksum": "sha1$f5a39e6eb18a0acb427c1997ad5a2588899fd777"
    },
    "transects_extended_geojson": {
        "class": "File",
        "location": "0510ab81113d6bfc0368c462edafba24fd1c369f",
        "size": 92335358,
        "basename": "transects_extended.geojson",
        "nameroot": "transects_extended",
        "nameext": ".geojson",
        "checksum": "sha1$0510ab81113d6bfc0368c462edafba24fd1c369f"
    },
    "transect_time_series_per_site": {
        "class": "Directory",
        "basename": "med_data"
    },
    "gee_key_json": "(secret-5a309c75-6baa-46fd-b04b-306030e2ecaa)",
    "niwa_tide_api_key": "(secret-1735ab33-5b80-41ca-a232-f18b1692e846)",
    "sds_slope": {
        "class": "File",
        "location": "85241311408d81b03b953464fdea3434ab8a3ac4",
        "size": 31359,
        "basename": "SDS_slope.py",
        "nameroot": "SDS_slope",
        "nameext": ".py",
        "contents": "##########################################################################################################\n# functions to estimate beach slope\n##########################################################################################################\n\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib import colorbar\nfrom matplotlib import lines\nfrom scipy import integrate as sintegrate\nfrom scipy import signal as ssignal\nfrom scipy import interpolate as sinterpolate\nfrom astropy.timeseries import LombScargle\nimport geopandas as gpd\nimport pytz\nfrom shapely import geometry\nimport pdb\nimport pyfes\n\n# plotting params\nplt.style.use('default')\nplt.rcParams['font.size'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['legend.fontsize'] = 1\n\n###################################################################################################\n# QA for CoastSat shorelines\n###################################################################################################\n    \ndef remove_duplicates(output):\n    \"\"\"\n    Function to remove from the output dictionnary entries containing shorelines for \n    the same date and satellite mission. This happens when there is an overlap \n    between adjacent satellite images.\n    \n    KV WRL 2020\n    \n    Arguments:\n    -----------\n        output: dict\n            contains output dict with shoreline and metadata\n        \n    Returns:    \n    -----------\n        output_no_duplicates: dict\n            contains the updated dict where duplicates have been removed\n        \n    \"\"\"\n    # remove duplicates\n    dates = output['dates'].copy()\n    # find the pairs of images that are within 5 minutes of each other\n    time_delta = 5*60 # 5 minutes in seconds\n    pairs = []\n    for i,date in enumerate(dates):\n        # dummy value so it does not match it again\n        dates[i] = pytz.utc.localize(datetime(1,1,1) + timedelta(days=i+1))\n        # calculate time difference\n        time_diff = np.array([np.abs((date - _).total_seconds()) for _ in dates])\n        # find the matching times and add to pairs list\n        boolvec = time_diff <= time_delta\n        if np.sum(boolvec) == 0:\n            continue\n        else:\n            idx_dup = np.where(boolvec)[0][0]\n            pairs.append([i,idx_dup])\n            \n    # if there are duplicates, only keep the longest shoreline\n    if len(pairs) > 0:\n        # initialise variables\n        output_no_duplicates = dict([])\n        idx_remove = []\n        # for each pair\n        for pair in pairs:\n            # check if any of the shorelines are empty\n            empty_bool = [(len(output['shorelines'][_]) == 0) for _ in pair]\n            if np.all(empty_bool): # if both empty remove both\n                idx_remove.append(pair[0])\n                idx_remove.append(pair[1])\n            elif np.any(empty_bool): # if one empty remove that one\n                idx_remove.append(pair[np.where(empty_bool)[0][0]])\n            else: # remove the shorter shoreline and keep the longer one\n                sl0 = geometry.LineString(output['shorelines'][pair[0]]) \n                sl1 = geometry.LineString(output['shorelines'][pair[1]])\n                if sl0.length >= sl1.length: idx_remove.append(pair[1])\n                else: idx_remove.append(pair[0])\n        # create a new output structure with all the duplicates removed\n        idx_remove = sorted(idx_remove)\n        idx_all = np.linspace(0, len(dates)-1, len(dates)).astype(int)\n        idx_keep = list(np.where(~np.isin(idx_all,idx_remove))[0])        \n        for key in output.keys():\n            output_no_duplicates[key] = [output[key][i] for i in idx_keep]\n        print('%d duplicates' % len(idx_remove))\n        return output_no_duplicates \n    else: \n        print('0 duplicates')\n        return output\n                \ndef remove_inaccurate_georef(output, accuracy):\n    \"\"\"\n    Function to remove from the output dictionnary entries containing shorelines that were mapped\n    on images with inaccurate georeferencing (RMSE > accuracy or flagged with -1)\n    \n    Arguments:\n    -----------\n        output: dict\n            contains the extracted shorelines and corresponding metadata\n        accuracy: int\n            minimum horizontal georeferencing accuracy (metres) for a shoreline to be accepted \n        \n    Returns:    \n    -----------\n        output_filtered: dict\n            contains the updated dictionnary\n        \n    \"\"\"\n    \n    # find indices of shorelines to be removed\n    idx = np.where(~np.logical_or(np.array(output['geoaccuracy']) == -1,\n                                  np.array(output['geoaccuracy']) >= accuracy))[0]\n    output_filtered = dict([])\n    for key in output.keys():\n        output_filtered[key] = [output[key][i] for i in idx]\n    print('%d bad georef' % (len(output['geoaccuracy']) - len(idx)))\n    return output_filtered\n\ndef transects_from_geojson(filename):\n    \"\"\"\n    Reads transect coordinates from a .geojson file.\n\n    Arguments:\n    -----------\n        filename: str\n            contains the path and filename of the geojson file to be loaded\n\n    Returns:\n    -----------\n        transects: dict\n            contains the X and Y coordinates of each transect.\n\n    \"\"\"\n\n    gdf = gpd.read_file(filename)\n    transects = dict([])\n    for i in gdf.index:\n        transects[gdf.loc[i,'name']] = np.array(gdf.loc[i,'geometry'].coords)\n\n    print('%d transects have been loaded' % len(transects.keys()))\n\n    return transects\n\ndef compute_intersection(output, transects, settings):\n    \"\"\"\n    Computes the intersection between the 2D mapped shorelines and the transects, to generate\n    time-series of cross-shore distance along each transect.\n\n    Arguments:\n    -----------\n        output: dict\n            contains the extracted shorelines and corresponding dates.\n        transects: dict\n            contains the X and Y coordinates of the transects (first and last point needed for each\n            transect).\n        settings: dict\n                along_dist: float\n                alongshore distance to caluclate the intersection (median of points\n                within this distance).\n                max_std: float\n                if the standard deviation of the points is above this threshold a nan is returned\n                max_range: float\n                if the range of the points is above this threshold a nan is returned                \n                min_val: float\n                largest negative value along transect (landwards of transect origin)\n                nan/max: str\n                'nan', 'max' or 'auto', how to deal with multiple intersections, \n                either put a nan or take the maximum (most seawards intersection),\n                or automatically decide based on the occurence of multiple intersections\n                (for example if there is a lagoon behind the beach, there are always 2 intersections)\n                prc_std: percentage of occurrence to use in 'auto' mode to switch from 'nan' to 'max'\n\n    Returns:\n    -----------\n        cross_dist: dict\n            time-series of cross-shore distance along each of the transects. These are not tidally\n            corrected.\n\n    \"\"\"\n\n    # initialise dictionary with intersections for each transect\n    cross_dist = dict([])\n\n    shorelines = output['shorelines']\n    along_dist = settings['along_dist']\n\n    # loop through each transect\n    for key in transects.keys():\n\n        # initialise variables\n        std_intersect = np.zeros(len(shorelines))\n        med_intersect = np.zeros(len(shorelines))\n        max_intersect = np.zeros(len(shorelines))\n        min_intersect = np.zeros(len(shorelines))\n        n_intersect = np.zeros(len(shorelines))\n\n        # loop through each shoreline\n        for i in range(len(shorelines)):\n\n            sl = shorelines[i]\n\n            # compute rotation matrix\n            X0 = transects[key][0,0]\n            Y0 = transects[key][0,1]\n            temp = np.array(transects[key][-1,:]) - np.array(transects[key][0,:])\n            phi = np.arctan2(temp[1], temp[0])\n            Mrot = np.array([[np.cos(phi), np.sin(phi)],[-np.sin(phi), np.cos(phi)]])\n\n            # calculate point to line distance between shoreline points and the transect\n            p1 = np.array([X0,Y0])\n            p2 = transects[key][-1,:]\n            d_line = np.abs(np.cross(p2-p1,sl-p1)/np.linalg.norm(p2-p1))\n            # calculate the distance between shoreline points and the origin of the transect\n            d_origin = np.array([np.linalg.norm(sl[k,:] - p1) for k in range(len(sl))])\n            # find the shoreline points that are close to the transects and to the origin\n            # the distance to the origin is hard-coded here to 1 km\n            idx_dist = np.logical_and(d_line <= along_dist, d_origin <= 1000)\n            idx_close = np.where(idx_dist)[0]\n\n            # in case there are no shoreline points close to the transect\n            if len(idx_close) == 0:\n                std_intersect[i] = np.nan\n                med_intersect[i] = np.nan\n                max_intersect[i] = np.nan\n                min_intersect[i] = np.nan\n                n_intersect[i] = np.nan\n            else:\n                # change of base to shore-normal coordinate system\n                xy_close = np.array([sl[idx_close,0],sl[idx_close,1]]) - np.tile(np.array([[X0],\n                                   [Y0]]), (1,len(sl[idx_close])))\n                xy_rot = np.matmul(Mrot, xy_close)\n                # remove points that are too far landwards relative to the transect origin (i.e., negative chainage)\n                xy_rot[0, xy_rot[0,:] < settings['min_val']] = np.nan\n\n                # compute std, median, max, min of the intersections\n                std_intersect[i] = np.nanstd(xy_rot[0,:])\n                med_intersect[i] = np.nanmedian(xy_rot[0,:])\n                max_intersect[i] = np.nanmax(xy_rot[0,:])\n                min_intersect[i] = np.nanmin(xy_rot[0,:])\n                n_intersect[i] = len(xy_rot[0,:])\n\n        # quality control the intersections using dispersion metrics (std and range)\n        condition1 = std_intersect <= settings['max_std']\n        condition2 = (max_intersect - min_intersect) <= settings['max_range']\n        condition3 = n_intersect > 2\n        idx_good = np.logical_and(np.logical_and(condition1, condition2), condition3)\n\n        # decide what to do with the intersections with high dispersion\n        if settings['nan/max'] == 'auto':\n            # compute the percentage of data points where the std is larger than the user-defined max\n            prc_over = np.sum(std_intersect > settings['max_std'])/len(std_intersect)\n            # if more than a certain percentage is above, use the maximum intersection\n            if prc_over > settings['prc_std']:\n                med_intersect[~idx_good] = max_intersect[~idx_good]\n                med_intersect[~condition3] = np.nan\n            # otherwise put a nan\n            else:\n                med_intersect[~idx_good] = np.nan\n\n        elif settings['nan/max'] == 'max':\n            med_intersect[~idx_good] = max_intersect[~idx_good]\n            med_intersect[~condition3] = np.nan\n\n        elif settings['nan/max'] == 'nan':\n            med_intersect[~idx_good] = np.nan\n\n        else:\n            raise Exception('the nan/max parameter can only be: nan, max or auto')\n\n        # store in dict\n        cross_dist[key] = med_intersect\n\n    return cross_dist\n\ndef reject_outliers(cross_distance, output, settings):\n    \"\"\"\n\n    Arguments:\n    -----------\n        cross_distance: dict\n            time-series of shoreline change\n        output: dict\n            mapped shorelines with metadata\n        settings: dict\n\n    Returns:\n    -----------\n        chain_dict: dict\n            contains the updated time-series of cross-shore distance with the corresponding dates\n\n    \"\"\"\n\n    chain_dict = dict([])\n\n    for i,key in enumerate(list(cross_distance.keys())):\n\n        chainage = cross_distance[key].copy()\n        if sum(np.isnan(chainage)) == len(chainage):\n            continue\n\n        # 1. remove nans and negative chainages\n        idx_nonan = np.where(~np.isnan(chainage))[0]\n        chainage1 = [chainage[k] for k in idx_nonan]\n        dates1 = [output['dates'][k] for k in idx_nonan]\n\n        # 3. remove outliers based on despiking [iterative method]\n        chainage3, dates3 = identify_outliers(chainage1, dates1, settings['max_cross_change'])\n\n        # fill with nans the indices to be removed from cross_distance\n        idx_kept = []\n        for date in output['dates']: idx_kept.append(date in dates3)\n        chainage[~np.array(idx_kept)] = np.nan\n        # store in chain_dict\n        chain_dict[key] = chainage\n\n        print('%s  - outliers removed %d'%(key, len(dates1) - len(dates3)))\n\n    return chain_dict\n\ndef identify_outliers(chainage, dates, cross_change):\n    \"\"\"\n    Remove outliers based on despiking [iterative method]\n\n    Arguments:\n    -----------\n    chainage: list\n        time-series of shoreline change\n    dates: list of datetimes\n        correspondings dates\n    cross_change: float\n        threshold distance to identify a point as an outlier\n\n    Returns:\n    -----------\n    chainage_temp: list\n        time-series of shoreline change without outliers\n    dates_temp: list of datetimes\n        dates without outliers\n\n    \"\"\"\n\n    # make a copy of the inputs\n    chainage_temp = chainage.copy()\n    dates_temp = dates.copy()\n\n    # loop through the time-series always starting from the start\n    # when an outlier is found, remove it and restart\n    # repeat until no more outliers are found in the time-series\n    k = 0\n    while k < len(chainage_temp):\n\n        for k in range(len(chainage_temp)):\n\n            # check if the first point is an outlier\n            if k == 0:\n                # difference between 1st and 2nd point in the time-series\n                diff = chainage_temp[k] - chainage_temp[k+1]\n                if np.abs(diff) > cross_change:\n                    chainage_temp.pop(k)\n                    dates_temp.pop(k)\n                    break\n\n            # check if the last point is an outlier\n            elif k == len(chainage_temp)-1:\n                # difference between last and before last point in the time-series\n                diff = chainage_temp[k] - chainage_temp[k-1]\n                if np.abs(diff) > cross_change:\n                    chainage_temp.pop(k)\n                    dates_temp.pop(k)\n                    break\n\n            # check if a point is an isolated outlier or in a group of 2 consecutive outliers\n            else:\n                # calculate the difference with the data point before and after\n                diff_m1 = chainage_temp[k] - chainage_temp[k-1]\n                diff_p1 = chainage_temp[k] - chainage_temp[k+1]\n                # remove point if isolated outlier, distant from both neighbours\n                condition1 = np.abs(diff_m1) > cross_change\n                condition2 = np.abs(diff_p1) > cross_change\n                # check that distance from neighbours has the same sign\n                condition3 = np.sign(diff_p1) == np.sign(diff_m1)\n                if np.logical_and(np.logical_and(condition1,condition2),condition3):\n                    chainage_temp.pop(k)\n                    dates_temp.pop(k)\n                    break\n\n                # check for 2 consecutive outliers in the time-series\n                if k >= 2 and k < len(chainage_temp)-2:\n\n                    # calculate difference with the data around the neighbours of the point\n                    diff_m2 = chainage_temp[k-1] - chainage_temp[k-2]\n                    diff_p2 = chainage_temp[k+1] - chainage_temp[k+2]\n                    # remove if there are 2 consecutive outliers (see conditions below)\n                    condition4 = np.abs(diff_m2) > cross_change\n                    condition5 = np.abs(diff_p2) > cross_change\n                    condition6 = np.sign(diff_m1) == np.sign(diff_p2)\n                    condition7 = np.sign(diff_p1) == np.sign(diff_m2)\n                    # check for both combinations (1,5,6 and ,2,4,7)\n                    if np.logical_and(np.logical_and(condition1,condition5),condition6):\n                        chainage_temp.pop(k)\n                        dates_temp.pop(k)\n                        break\n                    elif np.logical_and(np.logical_and(condition2,condition4),condition7):\n                        chainage_temp.pop(k)\n                        dates_temp.pop(k)\n                        break\n\n                    # also look for clusters of 3 outliers\n                    else:\n                        # increase the distance to make sure these are really outliers\n                        condition4b = np.abs(diff_m2) > 1.5*cross_change\n                        condition5b = np.abs(diff_p2) > 1.5*cross_change\n                        condition8 = np.sign(diff_m2) == np.sign(diff_p2)\n                        # if point is close to immediate neighbours but\n                        # the neighbours are far from their neighbours, point is an outlier\n                        if np.logical_and(np.logical_and(np.logical_and(condition4b,condition5b),\n                                                         np.logical_and(~condition1,~condition2)),\n                                                         condition8):\n                            print('*', end='')\n                            chainage_temp.pop(k)\n                            dates_temp.pop(k)\n                            break\n\n        # if one full loop is completed (went through all the time-series without removing outlier)\n        # then increment k to get out of the loop\n        k = k + 1\n\n\n    # return the time-series where the outliers have been removed\n    return chainage_temp, dates_temp\n\ndef plot_cross_distance(dates, cross_distance):\n    'plot the time-series of shoreline change from CoastSat'\n\n    for i,key in enumerate(cross_distance.keys()):\n        idx_nan = np.isnan(cross_distance[key])\n        chain = cross_distance[key][~idx_nan]\n        dates_temp = [dates[k] for k in np.where(~idx_nan)[0]]\n        if len(chain)==0 or sum(idx_nan) > 0.5*len(idx_nan): continue\n        fig,ax=plt.subplots(1,1,figsize=[12,3])\n        fig.set_tight_layout(True)\n        ax.grid(linestyle=':', color='0.5')\n        ax.plot(dates_temp, chain - np.mean(chain), '-o', ms=3, mfc='w', mec='C0')\n        ax.set(title='%s - %d points'%(key,len(chain)), ylabel='distance [m]', ylim=get_min_max_dict(cross_distance))\n\ndef get_min_max_dict(cross_distance):\n    'get min and max of a dictionary of time-series'\n    xmin = 1e10\n    xmax = -1e10\n    for key in cross_distance.keys():\n        ts = cross_distance[key] - np.nanmedian(cross_distance[key])\n        if np.nanmin(ts) < xmin:\n            xmin = np.nanmin(ts)\n        if np.nanmax(ts) > xmax:\n            xmax = np.nanmax(ts)\n    xmax = np.max([np.abs(xmin),np.abs(xmax)])\n    xmin = -np.max([np.abs(xmin),np.abs(xmax)])\n    return [xmin, xmax]\n\ndef get_min_max(y):\n    'get min and max of a time-series'\n    ymin = np.nanmin(y)\n    ymax = np.nanmax(y)\n    ymax = np.max([np.abs(ymin),np.abs(ymax)])\n    ymin = -np.max([np.abs(ymin),np.abs(ymax)])\n    return [ymin,ymax]\n\n###################################################################################################\n# Tide functions\n###################################################################################################\n\ndef compute_tide(coords,date_range,time_step,ocean_tide,load_tide):\n    'compute time-series of water level for a location and dates using a time_step'\n    # list of datetimes (every timestep)\n    dates = []\n    date = date_range[0]\n    while date <= date_range[1]:\n        dates.append(date)\n        date = date + timedelta(seconds=time_step)\n    # convert list of datetimes to numpy dates\n    dates_np = np.empty((len(dates),), dtype='datetime64[us]')\n    for i,date in enumerate(dates):\n        dates_np[i] = datetime(date.year,date.month,date.day,date.hour,date.minute,date.second)\n    lons = coords[0]*np.ones(len(dates))\n    lats = coords[1]*np.ones(len(dates))\n    # compute heights for ocean tide and loadings\n    tide, lp, _ = pyfes.evaluate_tide(ocean_tide,\n                                  dates_np,\n                                  lons,\n                                  lats,\n                                  num_threads=0)\n    load, load_lp, _ = pyfes.evaluate_tide(load_tide,\n                                        dates_np,\n                                        lons,\n                                        lats,\n                                        num_threads=0)\n    # sum up all components and convert from cm to m\n    tide_level = (tide + lp + load + load_lp)/100\n\n    return dates, tide_level\n\ndef compute_tide_dates(coords,dates,ocean_tide,load_tide):\n    'compute time-series of water level for a location and dates (using a dates vector)'\n    dates_np = np.empty((len(dates),), dtype='datetime64[us]')\n    for i,date in enumerate(dates):\n        dates_np[i] = datetime(date.year,date.month,date.day,date.hour,date.minute,date.second)\n    lons = coords[0]*np.ones(len(dates))\n    lats = coords[1]*np.ones(len(dates))\n    # compute heights for ocean tide and loadings\n    tide, lp, _ = pyfes.evaluate_tide(ocean_tide,\n                                  dates_np,\n                                  lons,\n                                  lats,\n                                  num_threads=0)\n    load, load_lp, _ = pyfes.evaluate_tide(load_tide,\n                                        dates_np,\n                                        lons,\n                                        lats,\n                                        num_threads=0)\n    # sum up all components and convert from cm to m\n    tide_level = (tide + lp + load + load_lp)/100\n\n    return tide_level\n\ndef find_tide_peak(dates,tide_level,settings):\n    'find the high frequency peak in the tidal time-series'\n    # create frequency grid\n    t = np.array([_.timestamp() for _ in dates]).astype('float64')\n    days_in_year = 365.2425\n    seconds_in_day = 24*3600\n    time_step = settings['n_days']*seconds_in_day\n    freqs = frequency_grid(t,time_step,settings['n0'])\n    # compute power spectrum\n    ps_tide,_,_ = power_spectrum(t,tide_level,freqs,[])\n    # find peaks in spectrum\n    idx_peaks,_ = ssignal.find_peaks(ps_tide, height=0)\n    y_peaks = _['peak_heights']\n    idx_peaks = idx_peaks[np.flipud(np.argsort(y_peaks))]\n    # find the strongest peak at the high frequency (defined by freqs_cutoff[1])\n    idx_max = idx_peaks[freqs[idx_peaks] > settings['freqs_cutoff']][0]\n    # compute the frequencies around the max peak with some buffer (defined by buffer_coeff)\n    freqs_max = [freqs[idx_max] - settings['delta_f'], freqs[idx_max] + settings['delta_f']]\n    # make a plot of the spectrum\n    fig = plt.figure()\n    fig.set_size_inches([12,4])\n    fig.set_tight_layout(True)\n    ax = fig.add_subplot(111)\n    ax.grid(linestyle=':', color='0.5')\n    ax.plot(freqs,ps_tide)\n    ax.set_title('$\\Delta t$ = %d days'%settings['n_days'], x=0, ha='left')\n    ax.set(xticks=[(days_in_year*seconds_in_day)**-1, (30*seconds_in_day)**-1, (16*seconds_in_day)**-1, (8*seconds_in_day)**-1],\n                   xticklabels=['1y','1m','16d','8d']);\n    # show top 3 peaks\n    for k in range(2):\n        ax.plot(freqs[idx_peaks[k]], ps_tide[idx_peaks[k]], 'ro', ms=4)\n        ax.text(freqs[idx_peaks[k]], ps_tide[idx_peaks[k]]+1, '%.1f d'%((freqs[idx_peaks[k]]**-1)/(3600*24)),\n                ha='center', va='bottom', fontsize=8, bbox=dict(boxstyle='square', ec='k',fc='w', alpha=0.5))\n    ax.axvline(x=freqs_max[1], ls='--', c='0.5')\n    ax.axvline(x=freqs_max[0], ls='--', c='0.5')\n    ax.axvline(x=(2*settings['n_days']*seconds_in_day)**-1, ls='--', c='k')\n    return freqs_max\n\ndef frequency_grid(time,time_step,n0):\n    'define frequency grid for Lomb-Scargle transform'\n    T = np.max(time) - np.min(time)\n    fmin = 1/T\n    fmax = 1/(2*time_step) # Niquist criterium\n    df = 1/(n0*T)\n    N = np.ceil((fmax - fmin)/df).astype(int)\n    freqs = fmin + df * np.arange(N)\n    return freqs\n\ndef power_spectrum(t,y,freqs,idx_cut):\n    'compute power spectrum and integrate'\n    model = LombScargle(t, y, dy=None, fit_mean=True, center_data=True, nterms=1, normalization='psd')\n    ps = model.power(freqs)\n    # integrate the entire power spectrum\n    E = sintegrate.simpson(ps, x=freqs)\n    if len(idx_cut) == 0:\n        idx_cut = np.ones(freqs.size).astype(bool)\n    # integrate only frequencies above cut-off\n    Ec = sintegrate.simpson(ps[idx_cut], x=freqs[idx_cut])\n    return ps, E, Ec\n\n###################################################################################################\n# Slope functions\n###################################################################################################\n\ndef range_slopes(min_slope, max_slope, delta_slope):\n    'create list of beach slopes to test'\n    beach_slopes = []\n    slope = min_slope\n    while slope < max_slope:\n        beach_slopes.append(slope)\n        slope = slope + delta_slope\n    beach_slopes.append(slope)\n    beach_slopes = np.round(beach_slopes,len(str(delta_slope).split('.')[1]))\n    return beach_slopes\n\ndef tide_correct(chain,tide_level,beach_slopes):\n    'apply tidal correction with a range of slopes'\n    tsall = []\n    for i,slope in enumerate(beach_slopes):\n        # apply tidal correction\n        tide_correction = (tide_level)/slope\n        ts = chain + tide_correction\n        tsall.append(ts)\n    return tsall\n\ndef plot_spectrum_all(dates_rand,composite,tsall,settings, title):\n    'plot the spectrum of the tidally-corrected time-series of shoreline change'\n    t = np.array([_.timestamp() for _ in dates_rand]).astype('float64')\n    seconds_in_day = 24*3600\n    days_in_year = 365.2425\n    time_step = settings['n_days']*seconds_in_day\n    freqs = frequency_grid(t,time_step,settings['n0'])\n    beach_slopes = range_slopes(settings['slope_min'], settings['slope_max'], settings['delta_slope'])\n\n    # make figure 1\n    fig = plt.figure()\n    fig.set_size_inches([12,5])\n    fig.set_tight_layout(True)\n    fig.suptitle(title, x=0.1, ha='left', fontweight='bold',\n     bbox=dict(boxstyle='square', ec='k',fc='w', alpha=0.5))\n    cmap = cm.get_cmap('RdYlGn')\n    color_list = cmap(np.linspace(0,1,len(beach_slopes)))\n    indices = np.arange(0,len(beach_slopes))\n    # axis labels\n    freq_1month = 1/(days_in_year*seconds_in_day/12)\n    xt = freq_1month/np.array([12,3,1, 20/(days_in_year/12), 16/(days_in_year/12)])\n    xl = ['1y', '3m', '1m', '20d', '16d']\n    # loop for plots\n    ax = fig.add_subplot(111)\n    ax.grid(which='major', linestyle=':', color='0.5')\n    ax.set(xticks=xt, xticklabels=xl, title='Power Spectrum of tidally-corrected time-series', ylabel='amplitude')\n    for i,idx in enumerate(indices):\n        # compute spectrum\n        ps,_,_ = power_spectrum(t,tsall[idx],freqs,[])\n        ax.plot(freqs, ps, '-', color=color_list[idx,:], lw=1)\n    # draw some references\n    ax.axvline(x=settings['freqs_max'][0], ls='--', c='0.5')\n    ax.axvline(x=settings['freqs_max'][1], ls='--', c='0.5')\n    ax.axvspan(xmin=settings['freqs_max'][0], xmax=settings['freqs_max'][1], color='0.85')\n    ax.axvline(x=(16*seconds_in_day)**-1, ls='--', c='k')\n\n    # make figure 2\n    fig = plt.figure()\n    fig.set_size_inches([12,5])\n    fig.set_tight_layout(True)\n    # axis labels\n    xt = 1./(np.flipud(np.arange(settings['n_days']*2,21,1))*24*3600)\n    xl = ['%d d'%(_) for _ in np.flipud(np.arange(settings['n_days']*2,21,1))]\n    # loop for plots\n    ax = fig.add_subplot(111)\n    ax.axvline(x=settings['freqs_max'][0], ls='--', c='0.5')\n    ax.axvline(x=settings['freqs_max'][1], ls='--', c='0.5')\n    ax.axvspan(xmin=settings['freqs_max'][0], xmax=settings['freqs_max'][1], color='0.85')\n    ax.grid(which='major', linestyle=':', color='0.5')\n    ax.set(xticks=xt, xticklabels=xl, ylabel='amplitude', title='Inset into the tidal peak frequency bands')\n    idx_interval = np.logical_and(freqs >= settings['freqs_max'][0], freqs <= settings['freqs_max'][1])\n    for i,idx in enumerate(indices):\n        # compute spectrum\n        ps, _,_ = power_spectrum(t,tsall[idx],freqs,[])\n        ax.plot(freqs[idx_interval], ps[idx_interval], '-', color=color_list[idx,:], lw=1)\n    # non-corrected time-series\n    ps,_,_ = power_spectrum(t,composite,freqs,[])\n    ax.plot(freqs[idx_interval], ps[idx_interval], '--', color='b', lw=1.5)\n    # add legend\n    nc_line = lines.Line2D([],[],ls='--', c='b', lw=1.5, label='non-corrected time-series')\n    ax.legend(handles=[nc_line], loc=2)\n\ndef integrate_power_spectrum(dates_rand,tsall,settings):\n    'integrate power spectrum at the frequency band of peak tidal signal'\n    t = np.array([_.timestamp() for _ in dates_rand]).astype('float64')\n    seconds_in_day = 24*3600\n    time_step = settings['n_days']*seconds_in_day\n    freqs = frequency_grid(t,time_step,settings['n0'])    \n    beach_slopes = range_slopes(settings['slope_min'], settings['slope_max'], settings['delta_slope'])\n    # integrate power spectrum\n    idx_interval = np.logical_and(freqs >= settings['freqs_max'][0], freqs <= settings['freqs_max'][1]) \n    E = np.zeros(beach_slopes.size)\n    for i in range(len(tsall)):\n        ps, _, _ = power_spectrum(t,tsall[i],freqs,[])\n        E[i] = sintegrate.simpson(ps[idx_interval], x=freqs[idx_interval])\n    # calculate confidence interval\n    delta = 0.0001\n    prc = settings['prc_conf']\n    f = sinterpolate.interp1d(beach_slopes, E, kind='linear')\n    beach_slopes_interp = range_slopes(settings['slope_min'],settings['slope_max']-delta,delta)\n    E_interp = f(beach_slopes_interp)\n    # find values below minimum + 5%\n    slopes_min = beach_slopes_interp[np.where(E_interp <= np.min(E)*(1+prc))[0]]\n    if len(slopes_min) > 1:\n        ci = [slopes_min[0],slopes_min[-1]]\n    else:\n        ci = [beach_slopes[np.argmin(E)],beach_slopes[np.argmin(E)]]\n    \n    # plot energy vs slope curve\n    fig = plt.figure()\n    fig.set_size_inches([12,4])\n    fig.set_tight_layout(True)\n    ax = fig.add_subplot(111)\n    ax.grid(linestyle=':', color='0.5')\n    ax.set(title='Energy in tidal frequency band', xlabel='slope values',ylabel='energy')\n    ax.plot(beach_slopes_interp,E_interp,'-k',lw=1.5)\n    cmap = cm.get_cmap('RdYlGn')\n    color_list = cmap(np.linspace(0,1,len(beach_slopes)))\n    for i in range(len(beach_slopes)): ax.plot(beach_slopes[i], E[i],'o',ms=8,mec='k',mfc=color_list[i,:])\n    ax.plot(beach_slopes[np.argmin(E)],np.min(E),'bo',ms=14,mfc='None',mew=2)\n    ax.text(0.65,0.85,\n            'slope estimate = %.3f\\nconf. band = [%.3f , %.3f]'%(beach_slopes[np.argmin(E)],ci[0],ci[1]),\n            transform=ax.transAxes,va='center',ha='left',\n            bbox=dict(boxstyle='round', ec='k',fc='w', alpha=0.5),fontsize=12)\n    ax.axhspan(ymin=np.min(E),ymax=np.min(E)*(1+prc),fc='0.7',alpha=0.5)\n    ybottom = ax.get_ylim()[0]\n    ax.plot([ci[0],ci[0]],[ybottom,f(ci[0])],'k--',lw=1,zorder=0)\n    ax.plot([ci[1],ci[1]],[ybottom,f(ci[1])],'k--',lw=1,zorder=0)\n    ax.plot([ci[0],ci[1]],[ybottom,ybottom],'k--',lw=1,zorder=0)\n\n    \n    return beach_slopes[np.argmin(E)], ci\n",
        "checksum": "sha1$85241311408d81b03b953464fdea3434ab8a3ac4"
    }
}